# Enterprise Implementation Guide - Phase 1 & 2

## Phase 1: Immediate Improvements (Week 1-2) âš¡

### 1.1 Add Request Rate Limiting
**File**: `src/middleware/rateLimit.js` (NEW)

```javascript
import rateLimit from 'express-rate-limit';

export const apiLimiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // 100 requests per windowMs
  message: 'Too many requests, please try again later.',
  standardHeaders: true, // Return rate limit info in the `RateLimit-*` headers
  legacyHeaders: false, // Disable the `X-RateLimit-*` headers
});

export const importLimiter = rateLimit({
  windowMs: 60 * 60 * 1000, // 1 hour
  max: 10, // 10 imports per hour max
  message: 'Maximum import limit reached. Please try again later.',
});

export const searchLimiter = rateLimit({
  windowMs: 1 * 60 * 1000, // 1 minute
  max: 30, // 30 searches per minute
  message: 'Search rate limit exceeded.',
});
```

**Add to**: `server.js`
```javascript
import { apiLimiter, importLimiter, searchLimiter } from "./src/middleware/rateLimit.js";

// Apply limiters
app.use('/auto-import-tickets', importLimiter);
app.use('/compose-reply', searchLimiter);
app.use('/debug-search', searchLimiter);
```

### 1.2 Add Input Validation
**File**: `src/middleware/validation.js` (NEW)

```javascript
export function validateImportRequest(req, res, next) {
  const { startDate, endDate } = req.body;
  
  if (!startDate || !endDate) {
    return res.status(400).json({
      error: 'Missing required fields',
      required: ['startDate', 'endDate']
    });
  }

  // Validate date format (YYYY-MM-DD)
  const dateRegex = /^\d{4}-\d{2}-\d{2}$/;
  if (!dateRegex.test(startDate) || !dateRegex.test(endDate)) {
    return res.status(400).json({
      error: 'Invalid date format. Use YYYY-MM-DD'
    });
  }

  const start = new Date(startDate);
  const end = new Date(endDate);
  
  if (start > end) {
    return res.status(400).json({
      error: 'startDate must be before endDate'
    });
  }

  // Limit date range to 90 days
  const maxRange = 90 * 24 * 60 * 60 * 1000;
  if (end - start > maxRange) {
    return res.status(400).json({
      error: 'Date range cannot exceed 90 days',
      requested: Math.ceil((end - start) / (24 * 60 * 60 * 1000)),
      maximum: 90
    });
  }

  next();
}

export function validateSearchRequest(req, res, next) {
  const { query } = req.body;
  
  if (!query || typeof query !== 'string') {
    return res.status(400).json({
      error: 'query must be a non-empty string'
    });
  }

  if (query.length > 1000) {
    return res.status(400).json({
      error: 'query cannot exceed 1000 characters',
      length: query.length,
      maximum: 1000
    });
  }

  next();
}
```

**Add to**: `src/routes/navbar.route.js`
```javascript
import { validateImportRequest } from "../middleware/validation.js";

router.post("/auto-import-tickets", validateImportRequest, autoImportTickets);
```

### 1.3 Add Response Compression
**File**: `server.js` - Add after express setup

```javascript
import compression from 'compression';

// Add compression middleware
app.use(compression());
```

**Update**: `package.json`
```json
{
  "dependencies": {
    "compression": "^1.7.4",
    "express-rate-limit": "^7.1.5"
  }
}
```

### 1.4 Add Request/Response Logging
**File**: `src/middleware/logging.js` (NEW)

```javascript
import fs from 'fs';
import path from 'path';

const logDir = 'logs';
if (!fs.existsSync(logDir)) {
  fs.mkdirSync(logDir);
}

export function requestLogger(req, res, next) {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = Date.now() - start;
    const log = {
      timestamp: new Date().toISOString(),
      method: req.method,
      path: req.path,
      status: res.statusCode,
      duration: `${duration}ms`,
      ip: req.ip
    };
    
    // Log to file
    fs.appendFileSync(
      path.join(logDir, `${new Date().toISOString().split('T')[0]}.log`),
      JSON.stringify(log) + '\n'
    );
  });
  
  next();
}
```

**Add to**: `server.js`
```javascript
import { requestLogger } from "./src/middleware/logging.js";

app.use(requestLogger);
```

---

## Phase 2: Scalability Enhancements (Week 3-4) ðŸ“ˆ

### 2.1 Implement Batch Processing with Queue

**File**: `src/services/queue.js` (NEW)

```javascript
import PQueue from 'p-queue';

class ImportQueue {
  constructor() {
    // Process max 2 imports concurrently, max 1 per second
    this.queue = new PQueue({
      concurrency: 2,
      interval: 1000,
      intervalCap: 1
    });
    
    this.activeImports = new Map();
  }

  async addImport(importId, importFunction) {
    return this.queue.add(async () => {
      this.activeImports.set(importId, {
        status: 'processing',
        startTime: Date.now(),
        progress: 0
      });

      try {
        const result = await importFunction();
        this.activeImports.set(importId, {
          status: 'completed',
          result,
          endTime: Date.now()
        });
        return result;
      } catch (error) {
        this.activeImports.set(importId, {
          status: 'failed',
          error: error.message,
          endTime: Date.now()
        });
        throw error;
      }
    });
  }

  getStatus(importId) {
    return this.activeImports.get(importId);
  }

  getQueueSize() {
    return this.queue.size;
  }
}

export const importQueue = new ImportQueue();
```

**Update**: `package.json`
```json
{
  "dependencies": {
    "p-queue": "^7.3.4"
  }
}
```

### 2.2 Implement Pagination for Large Datasets

**File**: `src/services/pagination.js` (NEW)

```javascript
export function createPaginator(items, pageSize = 50) {
  return {
    items,
    pageSize,
    totalPages: Math.ceil(items.length / pageSize),
    
    getPage(pageNumber) {
      const start = (pageNumber - 1) * pageSize;
      const end = start + pageSize;
      return {
        page: pageNumber,
        pageSize: this.pageSize,
        totalPages: this.totalPages,
        totalItems: items.length,
        items: items.slice(start, end)
      };
    },
    
    getAll() {
      const pages = [];
      for (let i = 1; i <= this.totalPages; i++) {
        pages.push(this.getPage(i));
      }
      return pages;
    }
  };
}

export async function* asyncPaginator(items, pageSize = 50) {
  for (let i = 0; i < items.length; i += pageSize) {
    yield {
      page: Math.floor(i / pageSize) + 1,
      items: items.slice(i, i + pageSize)
    };
  }
}
```

### 2.3 Add Caching Layer

**File**: `src/services/cache.js` (NEW)

```javascript
class Cache {
  constructor(maxSize = 1000, ttl = 3600000) { // 1 hour default
    this.data = new Map();
    this.maxSize = maxSize;
    this.ttl = ttl;
  }

  set(key, value, customTtl = null) {
    // Evict oldest entry if cache is full
    if (this.data.size >= this.maxSize) {
      const firstKey = this.data.keys().next().value;
      this.data.delete(firstKey);
    }

    this.data.set(key, {
      value,
      timestamp: Date.now(),
      ttl: customTtl || this.ttl
    });
  }

  get(key) {
    const entry = this.data.get(key);
    if (!entry) return null;

    // Check if expired
    if (Date.now() - entry.timestamp > entry.ttl) {
      this.data.delete(key);
      return null;
    }

    return entry.value;
  }

  invalidate(key) {
    this.data.delete(key);
  }

  clear() {
    this.data.clear();
  }

  getStats() {
    return {
      size: this.data.size,
      maxSize: this.maxSize,
      utilizationPercent: Math.round((this.data.size / this.maxSize) * 100)
    };
  }
}

export const searchCache = new Cache(500, 3600000); // 500 entries, 1 hour TTL
export const formFieldCache = new Cache(100, 86400000); // 100 entries, 24 hour TTL
```

### 2.4 Implement Streaming for Large Exports

**File**: `src/services/streaming.js` (NEW)

```javascript
import { Transform } from 'stream';

export function createJSONStreamTransform() {
  let isFirst = true;
  
  return new Transform({
    objectMode: true,
    transform(chunk, encoding, callback) {
      if (isFirst) {
        this.push('[');
        isFirst = false;
      } else {
        this.push(',');
      }
      this.push(JSON.stringify(chunk));
      callback();
    },
    flush(callback) {
      this.push(']');
      callback();
    }
  });
}

export function createCSVStreamTransform() {
  let isFirst = true;
  const headers = ['id', 'ticket_id', 'subject', 'status', 'priority', 'created_at'];
  
  return new Transform({
    objectMode: true,
    transform(chunk, encoding, callback) {
      if (isFirst) {
        this.push(headers.join(',') + '\n');
        isFirst = false;
      }
      
      const row = headers.map(h => {
        const value = chunk[h];
        // Escape quotes in CSV
        return typeof value === 'string' ? `"${value.replace(/"/g, '""')}"` : value;
      });
      
      this.push(row.join(',') + '\n');
      callback();
    }
  });
}
```

---

## Phase 3: Monitoring & Analytics (Week 5) ðŸ“Š

### 3.1 Add Performance Monitoring

**File**: `src/middleware/performance.js` (NEW)

```javascript
export function performanceMonitor(req, res, next) {
  const startTime = process.hrtime.bigint();
  const startMem = process.memoryUsage();

  res.on('finish', () => {
    const endTime = process.hrtime.bigint();
    const endMem = process.memoryUsage();
    
    const duration = Number(endTime - startTime) / 1000000; // Convert to ms
    const memoryDelta = (endMem.heapUsed - startMem.heapUsed) / 1024 / 1024; // MB

    console.log({
      endpoint: req.path,
      method: req.method,
      status: res.statusCode,
      duration_ms: duration.toFixed(2),
      memory_delta_mb: memoryDelta.toFixed(2),
      timestamp: new Date().toISOString()
    });
  });

  next();
}
```

### 3.2 Add Health Check with Metrics

**File**: `src/controllers/health.js` (NEW)

```javascript
import os from 'os';

export function getHealthMetrics() {
  const uptime = process.uptime();
  const memory = process.memoryUsage();
  const cpuUsage = process.cpuUsage();

  return {
    status: 'healthy',
    uptime: `${Math.floor(uptime / 60)} minutes`,
    memory: {
      heapUsed: `${Math.round(memory.heapUsed / 1024 / 1024)} MB`,
      heapTotal: `${Math.round(memory.heapTotal / 1024 / 1024)} MB`,
      external: `${Math.round(memory.external / 1024 / 1024)} MB`
    },
    cpu: {
      user: `${(cpuUsage.user / 1000).toFixed(2)} ms`,
      system: `${(cpuUsage.system / 1000).toFixed(2)} ms`
    },
    system: {
      platform: os.platform(),
      cpus: os.cpus().length,
      loadAverage: os.loadavg()
    },
    timestamp: new Date().toISOString()
  };
}
```

---

## Implementation Checklist

### Week 1 Priority:
- [ ] Install express-rate-limit and compression
- [ ] Implement rate limiting middleware
- [ ] Add input validation
- [ ] Add response compression
- [ ] Add request logging

### Week 2 Priority:
- [ ] Test all middleware under load
- [ ] Set up log rotation
- [ ] Add monitoring dashboard

### Week 3 Priority:
- [ ] Implement queue system
- [ ] Add pagination service
- [ ] Implement caching

### Week 4 Priority:
- [ ] Add streaming endpoints
- [ ] Optimize Pinecone queries
- [ ] Stress test with large datasets

### Week 5 Priority:
- [ ] Add performance monitoring
- [ ] Set up alerts
- [ ] Create SLA documentation

---

## Expected Performance After Improvements

| Metric | Before | After |
|--------|--------|-------|
| Requests/sec | 5-10 | 50-100 |
| Ticket capacity | 1000-5000 | 100,000+ |
| Response time | 2-5s | 100-500ms |
| Memory usage | 500MB | 200MB |
| Concurrent imports | 1 | 10+ |
| Max data range | 30 days | 90 days |

---

## Installation Instructions

1. **Install new dependencies:**
```bash
npm install express-rate-limit compression p-queue
```

2. **Create middleware directory:**
```bash
mkdir -p src/middleware
```

3. **Copy files from this guide:**
- Create `src/middleware/rateLimit.js`
- Create `src/middleware/validation.js`
- Create `src/middleware/logging.js`
- Create `src/middleware/performance.js`
- Create `src/services/queue.js`
- Create `src/services/pagination.js`
- Create `src/services/cache.js`
- Create `src/services/streaming.js`
- Create `src/controllers/health.js`

4. **Update server.js with new middleware** (see code examples above)

5. **Test each feature:**
```bash
# Test rate limiting
for i in {1..150}; do curl http://localhost:3000/health; done

# Test import with validation
curl -X POST http://localhost:3000/auto-import-tickets \
  -H "Content-Type: application/json" \
  -d '{"startDate": "2025-12-01", "endDate": "invalid"}'

# Test health metrics
curl http://localhost:3000/health/metrics
```

---

## Next Steps

After implementing Phase 1-3:
1. Load test with 100,000+ tickets
2. Set up horizontal scaling with Docker
3. Implement database replication
4. Set up CDN for static assets
5. Create customer dashboard for monitoring

Your app will then be **enterprise-grade and ready to sell!** ðŸš€
